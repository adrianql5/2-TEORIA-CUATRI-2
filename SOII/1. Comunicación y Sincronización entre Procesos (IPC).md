Cómo un proceso puede pasar información a otro, cómo hacer que dos o más procesos no se interpongan entre sí, cómo obtener la secuencia apropiada cuando hay dependencias presentes.

# 1.1 Condiciones de Carrera
Los procesos que trabajan en conjunto pueden **compartir cierto espacio de almacenamiento** en el que pueden leer y escribir datos. Este almacenamiento compartido puede estar tanto en la **RAM** (*en una estructura de datos*) o en un **archivo compartido**. 

Como ejemplo tenemos un **directorio de spooler** que se usa para imprimir archivos mediante el **demonio de impresión**. Cada ranura del spooler contiene el nombre de un archivo. Si hay dos variables compartidas `sal` que apunta al siguiente archivo a imprimir y `ent` que apunta a la siguiente ranura libre. Estas variables se pueden guardar en una archivo compartido para los procesos.

Pongamos que tenemos las ranuras 0-3 están vacías y las 4-6 llenas, y dos procesos desean poner en cola a un proceso para imprimirlo.

![[Pasted image 20250215133706.png]]
El proceso **A** lee `ent` y guarda el valor 7 en una variable local llamada `siguiente_ranura_libre`. Después pasa el proceso **B** que lee `ent` y recibe un 7 y lo guarda en su variable local. **B** almacena el nombre de su archivo en la ranura 7 y actualiza `ent` para que valga 8.

En cierto momento **A** sabe que para el su ranura libre es la 7 y sobreescribe el nombre del archivo escrito por **B**, por lo que el archivo de **B** nunca se imprime.

Situaciones como estas en donde dos o más procesos están leyendo o escribiendo algunos datos compartidos y el resultado final depende de quién se ejecuta y exactamente cuándo lo hace, se conocen como **condiciones de carrera**. **Muy complejo depurar el codigo**.

# 1.2 Regiones Criticas
Para evitar condiciones de carrera debemos prohibir que varios procesos elan y escriban los datos compartidos (*archivos, memoria compartida, etc.*) al mismo tiempo. Necesitamos **exclusión mutua**. En el ejemplo del Spooler el proceso **B** empezó a utilizar las variables compartidas antes de que **A** hubiese acabado con ellas.

**Región Crítica:** parte del programa en la que se accede a la memoria compartida. 

Si pudiéramos ordenar las cosas de manera que dos procesos nunca estuvieran en sus regiones críticas al mismo tiempo, evitaríamos las carreras. Aunque esto evita las condiciones de carrera, **no es suficiente para que los procesos en paralelo cooperen** de forma correcta y eficiente.

- No puede haber dos procesos de manera simultánea dentro de sus regiones críticas
- No pueden hacerse suposiciones acerca de las velocidades o el número de CPUs.
- Ningún proceso que se ejecute fuera de sus región crítica puede bloquear otros procesos.
- Ningún proceso tiene que esperar para siempre a entrar a su región crítica.

![[Pasted image 20250322112620.png]]

# 1.3 Exclusión mutua con espera ocupada
El objetivo es lograr la **exclusión mutua** de manera que mientras un proceso esté ocupada utilizando la memoria compartida en su región crítica, ningún otro proceso puede entrar a su región crítica y ocasionar problemas.

## 1.3.1 Deshabilitando interrupciones
En un sistema con un solo procesador, la solución más simple es hacer que cada proceso deshabilite todas las interrupciones justo después de entrar a su región crítica y las rehabilite justo después de salir. Así **evitamos las interrupciones de reloj** que marcan cuando la **CPU conmuta de un proceso a otro**. Esto evita que otro proceso intervenga

Sin embargo no es conveniente dar a los procesos de usuario el poder de desactivar las interrupciones. Y si tenemos más de una CPU, solo se ve afectada la CPU que ejecutó la instrucción disable, las demás se ejecutan correctamente y accediendo a la memoria compartida.

## 1.3.2 Variables de candado
**Solución software**. Si tenemos una variable compartida que es 0 inicialmente. Cuando un proceso desea entrar a su región crítica primero evalúa el candado. Si es 0, el proceso lo fija en 1 y entra a la región crítica. Si el candado ya es 1 sólo espera hasta que el candado sea 0. 

**Un 0 significa que ningún proceso está en la región crítica y un 1 significa que algún proceso está en su región crítica**. 

Sin embargo puede suceder lo mimo que en el Spooler. Si un proceso lee el candado y ve que es 0. Antes de que pueda fijar el candado a 1, otro procesos se planifica para ejecutarse y fija el candado a 1. Cuando el primer proceso se ejecuta de nuevo, también fija el candado a 1 y por lo tanto dos procesos se encontraran en sus regiones críticas al mismo tiempo.

## 1.3.3 Alternancia estricta
![[Pasted image 20250322114243.png]]
La variable **turno** lleva la cuenta acerca de que proceso le toca entrar a su región crítica y examinar o actualizar la memoria compartida. El **proceso 0** inspeccionar turno, descubre que es 0 y **entra a su region crítica**. El **proceso 1** ve que es 0 por lo que se queda evaluando turno de forma continua. **La acción a evaluar en forma continua de una variable hasta que aparezca cierto valor se conoce como espera ocupada.**

Esta espera ocupada se debe intentar evitar ya que desperdicia tiempo de cpu, solo se usa cuando se cree que la espera será corta. 

Cuando el proceso 0 sale de la región crítica establece turno a 1, para permitir que el proceso 1 entre a su región crítica. Suponga que el proceso 1 sale rápidamente de su región crítica, de manera que ambos procesos se encuentran en sus regiones no críticas, con turno establecido en 0. Ahora el proceso 0 ejecuta todo su ciclo rápidamente, saliendo de su región crítica y estableciendo turno a 1. En este punto, turno es 1 y ambos procesos se están **ejecutando en sus regiones no críticas**.

De repente, el proceso 0 termina en su región no crítica y regresa a la parte superior de su ciclo. Por desgracia no puede entrar a su región crítica ahora, debido a que turno es 1 y el proceso 1 está ocupado con su región no crítica. El proceso 0 espera en su ciclo while hasta que el pro- ceso 1 establezca turno a 0. Dicho en forma distinta, tomar turnos no es una buena idea cuando uno de los procesos es mucho más lento que el otro. Esta situación viola la condición 3 antes establecida:

- Ningún proceso que se ejecute fuera de sus región crítica puede bloquear otros procesos.

## 1.3.4 Solución de Peterson
![[Pasted image 20250322121723.png]]
Antes de utilizar las variables compartidas (es decir, antes de entrar a su región crítica), cada proceso llama a `entrar_region` con su propio número de proceso como parámetro. Esta llamada hará que espera, si es necesario, hasta que sea seguro entrar. 

Cuando termina de usar las **variables compartidas** llama a `salir_region` para indicar que ha terminado y permitir que los demás procesos entren, si así lo desean.

El **proceso 0** llama a `entrar_region`. Indica su interés estableciendo su elemento del arreglo y fija **turno a 0**. Como el proceso 1 no está interesado, `entrar_region` regresa de inmediato. Si ahora el **proceso 1** hace una llamada a `entrar_region`, se quedará ahí hasta que interesado[0] sea **FALSE**, un evento que sólo ocurre cuando el **proceso 0** llama a `salir_region` para salir de la región crítica.

Ahora considere el caso en el que ambos procesos llaman a `entrar_region` casi en forma simultánea. Ambos almacenarán su número de proceso en **turno**. Cualquier almacenamiento que se haya realizado al último es el que cuenta; **el primero se sobrescribe y se pierde**. Suponga que el **proceso 1** almacena al último, por lo que **turno es 1**. Cuando ambos procesos llegan a la instrucción `while`, el **proceso 0 la ejecuta 0** **veces** **y entra a su región crítica**. El **proceso 1 itera y no entra a su región crítica sino hasta que el proceso 0 sale de su región crítica**.

## 1.3.5 Instrucción TSL
La instrucción **TSL (Test and Set Lock)** es una instrucción especial de bajo nivel que se usa en sistemas multiprocesador para garantizar la exclusión mutua en regiones críticas. Se trata de una operación atómica que ayuda a evitar **condiciones de carrera** cuando varios procesos intentan acceder a una variable compartida.

- **Lee el valor** de una variable de memoria compartida (llamada **candado**) y lo guarda en un registro.
- **Fija la variable** candado a un valor distinto de 0 (por ejemplo, 1) en memoria, asegurando que otros procesos no puedan modificarla.
- **Bloquea el bus de memoria** temporalmente mientras ejecuta estas dos operaciones, asegurando que ningún otro procesador pueda acceder a la variable hasta que la instrucción termine.

Es una alternativa para evitar condiciones de carrera es **deshabilitar interrupciones** antes de acceder a la región crítica. Sin embargo, esto **no funciona en sistemas multiprocesador** porque:
- Deshabilitar interrupciones en un procesador **no afecta a los demás**.
- Otro procesador puede leer la variable compartida y modificarla mientras el primero aún está ejecutando su código.

![[Pasted image 20250322123107.png]]


# 1.4 Dormir y Despertar
Para evitar la espera ocupada, se introduce el mecanismo de **Dormir y Despertar** con dos primitivas de comunicación entre procesos:

1. **`sleep()`** → Suspende el proceso actual **hasta que otro proceso lo despierte**.
2. **`wakeup(proceso)`** → Despierta un proceso suspendido, permitiéndole continuar su ejecución.

Esto significa que **si un proceso no puede entrar a la región crítica, en lugar de quedarse en un bucle gastando CPU, simplemente se suspende hasta que otro proceso lo despierte**.

## 1.4.1 El problema del productor-consumidor
Dos procesos comparten un **buffer compartido de tamaño fijo**. El productor coloca información en el buffer y el consumidor la saca. 

El problema surge cuando el productor desea colocar un nuevo elemento en el buffer, pero este ya está lleno. La solución es que se vaya a dormir y se despierte cuando el consumidor haya quitado uno o mas elementos. Lo mismo cuando el buffer esta vacío y el consumidor quiere retirar elementos.

Parece simple, pero genera los mismos **tipos de condiciones de carrera** del directorio de Spooler. Necesitamos la variable cuenta para saber llevar la cuenta de elementos en el buffer. 

![[Pasted image 20250322124847.png]]

Los procedimientos `insertar_elemento` y `quitar_elemento`, que no se muestran aquí se encargan de colocar elementos en el buffer y sacarlos del mismo. 

Debido a que el acceso a cuenta no está restringido. Es posible que ocurra la condición de carrera en la siguiente situación: **el buffer está vacío y el consumidor acaba de leer cuenta para ver si es 0**. En ese instante, el planificador decide detener al consumidor en forma temporal y empieza a ejecutar el productor.

El productor inserta un elemento en el buffer, incrementa cuenta y observa que ahora es 1. Razonando que cuenta era antes 0, y que por ende el consumidor debe estar dormido, el productor llama a wakeup. Sin embargo, el consumidor no está lógicamente dormido, por lo que la señal para despertarlo se pierde. Cuando es turno de que se ejecute el consumidor, evalúa el valor de cuenta en que leyó antes, encuentra que es 0 y pasa a dormirse. Tarde o temprano el **productor llenará el buffer y también pasará a dormirse**, quedando ambos dormidos para siempre.

La esencia del problema aquí es que una señal que se envía para despertar a un proceso que no está dormido (todavía) se pierde. Si no se perdiera, todo funcionaría. 

# 1.5 Semáforos
**Semáforo**, variable entera para contar el número de señales de **wakeup**, guardadas para un uso futuro. Almacena el valor 0, indicando que no se guardaron señales de despertar o algún valor positivo si estuvieran pendientes una o más señales de **wakeup**.

Tenemos dos operaciones, `down` y `up` (generalizaciones de `sleep` y `wakeup`). La operación `down` comprueba si el valor es mayor que 0. De ser así, disminuye el valor(utiliza una señal de despertar almacenada) y sólo continúa.

Si el valor es 0, el proceso se pone a dormir sin completar la operación `down` por el momento. Las acciones de comprobar el valor, modificarlo y posiblemente dormir, se realizan en conjunto como una sola **acción atómica** indivisible. 

Una vez que empieza la operación de semáforo, ningún otro proceso podrá accede al semáforo sino hasta que la operación se haya completado o bloqueado. Esto resuelve problemas de sincronización y evita condiciones de carrera.

La operación `up` incrementa el valor del semáforo direccionado. Si uno o más procesos estaban inactivos en ese semáforo, sin poder completar una operación `down` anterior, el sistema selecciona uno de ellos (al azar) y permite que complete su operación `down`.

## 1.5.1 Resolver el problema del productor-consumidor mediante el uso de semáforos

En este caso, se utilizan tres semáforos para coordinar el acceso al búfer compartido:

- **Semáforo `llenas`**: Cuenta cuántas ranuras del búfer están ocupadas:    
    - Inicialmente es 0 porque el búfer comienza vacío.
    - Se incrementa (`up`) cuando el productor coloca un elemento.
    - Se decrementa (`down`) cuando el consumidor retira un elemento.
    
- **Semáforo `vacías`**: Cuenta cuántas ranuras del búfer están disponibles. 
    - Se inicializa con el tamaño total del búfer (porque al inicio está completamente vacío).
    - Se decrementa (`down`) cuando el productor coloca un elemento.
    - Se incrementa (`up`) cuando el consumidor extrae un elemento.
    
- **Semáforo `mutex`**: Asegura que solo un proceso (productor o consumidor) acceda al búfer a la vez.
    - Se inicializa en 1 (se trata de un semáforo binario).
    - Se usa para garantizar **exclusión mutua**:
        - El productor lo adquiere (`down`) antes de escribir y lo libera (`up`) después.
        - El consumidor lo adquiere (`down`) antes de leer y lo libera (`up`) después.


## **Implementación de `up` y `down`**
Las operaciones `up` y `down` deben ser **atómicas** (indivisibles). Para garantizar esto, el sistema operativo suele deshabilitar las interrupciones brevemente mientras se realiza la operación. En sistemas con múltiples CPUs, se protege el semáforo con una variable de candado y **operaciones atómicas** como `TSL` (Test and Set Lock) o `XCHG` para evitar que varias CPUs lo modifiquen al mismo tiempo.



## **Flujo de ejecución**
![[Pasted image 20250324164420.png]]

## **Uso de semáforos en manejo de interrupciones**

Los semáforos también pueden sincronizar procesos con dispositivos de E/S. Cada dispositivo de E/S puede tener un semáforo inicializado en 0.

- Cuando un proceso inicia una operación de E/S, hace `down` en el semáforo y se bloquea.
    
- Cuando la interrupción de E/S ocurre, el manejador hace `up` en el semáforo, despertando al proceso.
    

Esto evita que los procesos hagan espera ocupada y permite que el planificador de CPU ejecute otras tareas mientras espera la E/S.

### Funciones clave

|Función|Descripción|
|---|---|
|`sem_init()`|Inicializa un semáforo.|
|`sem_wait()`|Decrementa el semáforo (espera si es 0).|
|`sem_post()`|Incrementa el semáforo (libera un recurso).|
|`sem_destroy()`|Libera recursos del semáforo.|
# 1.6 Mutexes
Los **mutexes** son buenos para administrar la exclusión mutua para cierto recurso compartido o pieza de código. 

Un **mutex** es una variable que puede estar en uno de dos estados: **abierto** (*desbloqueado*) o **cerrado** (*bloqueado*). En consecuencia, se requiere sólo 1 bit para representarla, pero en la práctica se utiliza con frecuencia un entero, en donde **0 indica que está abierto** y todos los demás valores indican que está **cerrado**.

Cuando un hilo necesita acceso a una **región crítica**, llama a `mutex_lock`. Si el mutex está actualmente abierto, la llamada tiene éxito y el **hilo llamador puede entrara la región crítica**.

Si el mutex ya **se encuentra cerrado**, el hilo que hizo la llamada se **bloquea** hasta que el hilo que está en la región crítica termine y llame a `mutex_unlock`. Si se bloquean varios hilos por el mutex, se selecciona uno de ellos al azar y se permite que adquiera el mutex.

`mutex_lock` es similar al código de `entrar_region`pero la diferencia es que en `entrar_region`, cuando no puede entrar, continúa evaluando el mutex en forma repetida (*espera ocupada*). Cuando el reloj expire algún otro proceso se programará para ejecutarlo. Cuando tenemos **hilos** no hay reloj, por lo que si empleamos la **espera ocupada** lo intentará adquirir de forma indefinida y nunca adquirira un mutex debido a que nunca permitirá que algún otro hilo se ejecute y libere el mutex. Entonces por eso `mutex_lock` llama a `thread_yield` para darle CPU a otro hilo.

Como se llama al `thread_yield` en el espacio de usuario es muy rápido y ni `mutex_lock` ni `mutex_unlock` requieren de llamadas al kernel.


## Mutexes (Mutual Exclusion Locks)
Los **mutexes** sirven para proteger **regiones críticas** en las que solo un hilo debe acceder a la vez, evitando que múltiples hilos modifiquen datos compartidos simultáneamente y generen condiciones de carrera.


|**Función**|**Descripción**|
|---|---|
|`pthread_mutex_init`|Crea un mutex antes de usarlo|
|`pthread_mutex_destroy`|Destruye un mutex cuando ya no se necesita|
|`pthread_mutex_lock`|Bloquea un mutex (si ya está bloqueado, el hilo se bloquea hasta que se libere)|
|`pthread_mutex_trylock`|Intenta bloquear un mutex, pero si está ocupado, **no bloquea el hilo** (retorna error en su lugar)|
|`pthread_mutex_unlock`|Libera un mutex, permitiendo que otro hilo lo adquiera|

**Ejemplo:**
```c
pthread_mutex_t lock;  // Declaración del mutex

// Inicialización antes de usar
pthread_mutex_init(&lock, NULL);

// Bloquear el mutex antes de entrar a la región crítica
pthread_mutex_lock(&lock);
    // 🔹 Sección crítica (modificación de recursos compartidos)
pthread_mutex_unlock(&lock);  // Liberar el mutex

// Al finalizar, destruir el mutex
pthread_mutex_destroy(&lock);
```

**Regla de oro:** Cada `pthread_mutex_lock` **debe** tener un `pthread_mutex_unlock` para evitar bloqueos permanentes.


## Variables de Condición
Los **mutexes** protegen el acceso a una región crítica, pero no son suficientes si un hilo debe **esperar a que ocurra una condición específica**. Para eso, se usan **variables de condición** junto con los mutexes.

| **Función**              | **Descripción**                                                                  |
| ------------------------ | -------------------------------------------------------------------------------- |
| `pthread_cond_init`      | Crea una variable de condición                                                   |
| `pthread_cond_destroy`   | Destruye una variable de condición                                               |
| `pthread_cond_wait`      | Bloquea el hilo hasta que otra señal lo despierte (**debe usarse con un mutex**) |
| `pthread_cond_signal`    | Despierta **un solo** hilo bloqueado en la variable de condición                 |
| `pthread_cond_broadcast` | Despierta **todos** los hilos bloqueados en la variable de condición             |
| 2                        |                                                                                  |
**Ejemplo:**
```c
pthread_mutex_t lock = PTHREAD_MUTEX_INITIALIZER;
pthread_cond_t cond = PTHREAD_COND_INITIALIZER;
int buffer_disponible = 0;  // 0 = vacío, 1 = lleno

void *productor(void *arg) {
    pthread_mutex_lock(&lock);
    while (buffer_disponible == 1) {
        pthread_cond_wait(&cond, &lock);  // Espera hasta que el buffer esté vacío
    }
    // 🔹 Producir algo
    buffer_disponible = 1;
    pthread_cond_signal(&cond);  // Despertar al consumidor
    pthread_mutex_unlock(&lock);
    return NULL;
}

void *consumidor(void *arg) {
    pthread_mutex_lock(&lock);
    while (buffer_disponible == 0) {
        pthread_cond_wait(&cond, &lock);  // Espera hasta que haya algo en el buffer
    }
    // 🔹 Consumir algo
    buffer_disponible = 0;
    pthread_cond_signal(&cond);  // Despertar al productor
    pthread_mutex_unlock(&lock);
    return NULL;
}
```

# 1.7 Monitores
Imagina que en el código de los semáforos llamamos antes al `down` de la variable `mutex` que al de la variable `vacías` en el código del `productor`. Si el buffer estuviera completamente lleno el productor se bloquearía, con `mutex = 0`. En consecuencia, la próxima vez que el consumidor tratara de acceder al buffer, realizaría una operación `down` en `mutex` y se bloquearía. Ambos procesos permanecerían bloqueados de manera indefinida y no se realizaría trabajo. Esto es un **interbloqueo**.



## 1.7.1 ¿Qué es un Monitor?
Un **monitor** es una **abstracción de alto nivel** para la **sincronización de procesos o hilos**, que **combina exclusión mutua automática con estructuras de datos y procedimientos protegidos**.

> Esencialmente, es un **módulo o paquete** que agrupa:

- Variables compartidas
- Procedimientos para acceder a esas variables
- Reglas que garantizan que **sólo un proceso/hilo** esté activo dentro del monitor a la vez

### **Objetivo**

- **Evitar condiciones de carrera** al controlar el acceso concurrente a datos compartidos.
- **Simplificar el diseño y la implementación** de programas concurrentes al automatizar la exclusión mutua.


## 1.7.2. ¿Cómo funciona un Monitor?
Sólo **un proceso o hilo** puede estar ejecutando dentro del monitor en un momento dado. 

Si otro proceso intenta entrar mientras hay uno activo, **se bloquea automáticamente** hasta que el monitor esté libre.    

_Analogía_: Piensa en un baño de una sola cabina. Si alguien está adentro, el siguiente tiene que esperar fuera. La cerradura (mutex) está gestionada automáticamente.


Los monitores son una **construcción del lenguaje**. El compilador reconoce cuándo se está accediendo a un monitor y **genera automáticamente** el código necesario para bloquear y desbloquear.
 
A diferencia de los semáforos (que son una herramienta de bajo nivel), los monitores son más **seguros y fáciles de usar**, porque el programador no necesita preocuparse por errores como olvidar un `unlock`.
  

Lenguajes como **Java** tienen soporte para monitores (a través de `synchronized`, `wait()`, y `notify()`).


## 1.7.3 Variables de condición
La exclusión mutua no es suficiente. A veces un proceso necesita **esperar hasta que se cumpla una condición** para poder continuar.

**Productor-Consumidor**
- El productor no puede insertar en el búfer si está lleno.
- El consumidor no puede eliminar si el búfer está vacío.

**Solución: variables de condición**
- Son mecanismos dentro de los monitores que permiten a un proceso **bloquearse activamente** cuando no puede continuar.
- Usan dos operaciones clave:
    - `wait(cond)`: bloquea el proceso hasta que otro haga `signal(cond)`
    - `signal(cond)`: despierta a **uno** de los procesos bloqueados en `cond`


_Importante_: Las variables de condición **no almacenan señales**. Si haces un `signal` y no hay nadie esperando, la señal **se pierde**.


## 1.7.4 Estrategias de Desbloqueo: ¿quién sigue?
Cuando un proceso hace `signal(cond)`, ¿quién debe ejecutarse?

1. **Hoare**: El proceso que fue despertado **entra inmediatamente al monitor**, suspendiendo al señalizador.  
2. **Brinch Hansen** (preferida): El señalizador **sale del monitor inmediatamente** después del `signal`, y entonces entra el otro proceso.
3. **Tercera opción (intermedia)**: El señalizador termina completamente y luego entra el otro.

📌 En la práctica, se suele usar la segunda (Brinch Hansen) por su **simplicidad de implementación**.


## 1.7.5 Ejemplo: Productor-Consumidor con Monitores

```pascal
monitor ProductorConsumidor
    condition llenas, vacias;
    integer cuenta;

    procedure insertar(elemento: integer);
    begin
        if cuenta = N then wait(llenas);
        insertar_elemento(elemento);
        cuenta := cuenta + 1;
        if cuenta = 1 then signal(vacias);
    end;

    function eliminar: integer;
    begin
        if cuenta = 0 then wait(vacias);
        eliminar := eliminar_elemento;
        cuenta := cuenta - 1;
        if cuenta = N - 1 then signal(llenas);
    end;

    cuenta := 0;
end monitor;
```

- `cuenta` lleva el número de elementos en el búfer. 
- Las variables de condición `llenas` y `vacias` ayudan a coordinar.
- Solo **un proceso a la vez** puede ejecutar `insertar` o `eliminar`.


## 1.7.6 Comparación con Semáforos y Sleep/Wakeup

**Problemas de `sleep()` y `wakeup()`**
- Son vulnerables a **condiciones de carrera**.
- Si `wakeup` ocurre **antes** de `sleep`, el proceso **se duerme para siempre**.

Los monitores **evitan estos errores** porque:
- Garantizan que `wait()` se ejecute **dentro** de una sección crítica. 
- Solo un proceso entra al monitor, por lo que no puede haber interferencia al hacer `wait`.

## 1.7.7 Limitaciones de los Monitores
- **Dependencia del lenguaje**: requieren soporte a nivel de compilador. Lenguajes como C no los soportan directamente.
- **Inutilizables en sistemas distribuidos**: están diseñados para **memoria compartida**, no para CPUs conectadas por red.
- **No permiten compartir datos entre máquinas**.

# 1.8 Paso de Mensajes
El **pasaje de mensajes** es un **método de comunicación entre procesos** (ya sea en el mismo sistema o en sistemas distribuidos), que **no depende de memoria compartida**. Es especialmente útil cuando:

- Los procesos están en **máquinas diferentes**.
- **No existe memoria común** entre procesos.
- Se busca una solución más **estructurada y segura** para la comunicación.


Funciona con dos operaciones básicas (*llamadas al sistema*):

- `send(destino, &mensaje);` → Envía un mensaje al proceso destino. (kill) 
- `receive(origen, &mensaje);` → Recibe un mensaje del proceso origen. (pause o manejadores)


Estas llamadas permiten a los procesos **enviar y recibir datos sin compartir memoria**, y **el sistema operativo gestiona** los detalles como la sincronización o el bloqueo si no hay mensajes disponibles.

|Mecanismo|Requiere Memoria Compartida|¿A qué nivel opera?|Soporte en Lenguaje|Comunicación de datos|
|---|---|---|---|---|
|**Semáforo**|Sí|Bajo nivel (llamada al sistema)|No, se implementan con librerías o ensamblador|No (sólo sincronización)|
|**Monitor**|Sí|Nivel del lenguaje (estructuras)|Sí, integrado en lenguajes como Java|No (principalmente sincronización)|
|**Pasaje de mensajes**|No|Llamada al sistema|No requiere integración en el lenguaje|**Sí, permite transferencia de datos**|

**Semáforos y monitores** se usan principalmente para **exclusión mutua** (sincronización).

**El pasaje de mensajes**, además de sincronizar, permite **transferir datos directamente** entre procesos.


### Problema del Productor-Consumidor
Con monitores (como en Java), el productor y el consumidor **acceden a un búfer compartido** y sincronizan su acceso usando `wait()` y `notify()` dentro de un objeto monitor.

Con el pasaje de mensajes **no hay búfer compartido**. El productor y el consumidor **se envían mensajes entre sí**, como en este flujo:

1. El **consumidor** envía `N` mensajes vacíos al **productor**.
2. Cada vez que el **productor** tiene un elemento:
    - Espera un mensaje vacío (`receive`).
    - Lo llena y lo envía al consumidor (`send`).
3. El **consumidor**:
    - Recibe un mensaje lleno.
    - Extrae el elemento.
    - Envía de vuelta un mensaje vacío.

Este ciclo permite **sincronización y transmisión de datos**, sin necesidad de memoria compartida.

En sistemas con **múltiples CPUs con memoria separada**, **no se puede usar memoria compartida**, por lo tanto ni semáforos ni monitores sirven. 

El pasaje de mensajes **sí puede funcionar**, ya que:  
- No requiere acceso a una misma memoria.
- Es naturalmente compatible con redes. 
- Permite manejar **perdidas, duplicados y errores** en la comunicación (por ejemplo, usando ACKs y números de secuencia).

### Cuestiones de diseño

1. **Mensajes perdidos:** se puede usar un sistema de  acuses de recibo (ACK).
2. **Duplicados:** se usan **números de secuencia** para detectarlos.
3. **Autenticación:** ¿cómo saber que el servidor con el que se habla es legítimo?
4. **Rendimiento:** es más lento que acceso a memoria compartida, pero se puede optimizar (p. ej., usando registros en lugar de copiar bloques grandes de memoria).
5. **Direccionamiento:** se pueden enviar mensajes a:
- Procesos específicos.  
- **Buzones**: estructura intermediaria para mensajes.

### Tipos de implementación
- **Con búferes (buzones):** los mensajes se almacenan hasta que se reciben.
- **Sin búfer (encuentro):** el envío y recepción ocurren al mismo tiempo. Si uno ocurre primero, se bloquea hasta que el otro proceso esté listo.

# 1.9 Barreras
Una **barrera** es una forma de sincronización colectiva. Es decir:

> “Ningún proceso (o hilo) puede avanzar más allá de cierto punto, hasta que **todos** los demás procesos también hayan llegado a ese mismo punto”.

Ideal para algoritmos que se ejecutan por fases, como los de simulación física o cálculos sobre matrices grandes.

Imagina 4 hilos (`A`, `B`, `C`, `D`). Cada uno trabaja de forma independiente en su pedazo de tarea. Al final de cada fase, todos hacen una llamada como `barrier()`.

- El primero que llega se queda esperando.
- Lo mismo el segundo y el tercero.
- El último que llega libera a todos.

Así todos comienzan la siguiente fase sincronizados.


## 1.9.1 ¿Cómo se implementan las barreras en C?
En C, podemos hacer esto manualmente con **semáforos y contadores**, pero la forma más sencilla y moderna es usando la **API de hilos de POSIX (pthreads)**, que incluye **barreras nativas** (`pthread_barrier_t`).


```c
#include <stdio.h>
#include <pthread.h>

#define NUM_THREADS 4
#define NUM_ITERATIONS 3

pthread_barrier_t barrier;

void* trabajar(void* arg) {
    int id = *(int*)arg;
    for (int fase = 0; fase < NUM_ITERATIONS; fase++) {
        // Simula trabajo
        printf("Hilo %d: Fase %d comenzada\n", id, fase);
        // Aquí podrías poner cálculos reales

        // Barrera: todos deben terminar esta fase antes de continuar
        pthread_barrier_wait(&barrier);

        // Simula el inicio de una nueva fase
        printf("Hilo %d: Fase %d completada\n", id, fase);
    }
    return NULL;
}

int main() {
    pthread_t hilos[NUM_THREADS];
    int ids[NUM_THREADS];

    // Inicializa la barrera para NUM_THREADS
    pthread_barrier_init(&barrier, NULL, NUM_THREADS);

    // Crea los hilos
    for (int i = 0; i < NUM_THREADS; i++) {
        ids[i] = i;
        pthread_create(&hilos[i], NULL, trabajar, &ids[i]);
    }

    // Espera a que terminen
    for (int i = 0; i < NUM_THREADS; i++) {
        pthread_join(hilos[i], NULL);
    }

    // Destruye la barrera
    pthread_barrier_destroy(&barrier);

    return 0;
}
```


Detalles importantes:
- `pthread_barrier_init(&barrier, NULL, NUM_THREADS)`  
    → Inicializa la barrera para que se desbloquee cuando hayan llegado los `NUM_THREADS` hilos.
- `pthread_barrier_wait(&barrier)`  
    → El hilo que llama aquí se bloquea hasta que todos los demás también lo llamen.    
- `pthread_barrier_destroy(&barrier)`  
    → Limpieza de recursos.

